global:
  metadata:
    name: "{{ .ClusterName }}"
    description: "E2E Test cluster"
    organization: "{{ .Organization }}"

  connectivity:
    baseDomain: gaws.gigantic.io
    availabilityZoneUsageLimit: 3
    dns:
      mode: public
    network:
      vpcCidr: 10.{{ index .ExtraValues "CIDRSecondOctet" }}.0.0/18
    proxy:
      enabled: true
      httpProxy: http://proxy.operations.awsprod.gigantic.io:4000
      httpsProxy: http://proxy.operations.awsprod.gigantic.io:4000
    subnets:
      - cidrBlocks:
          - availabilityZone: a
            cidr: 10.{{ index .ExtraValues "CIDRSecondOctet" }}.0.0/20
          - availabilityZone: b
            cidr: 10.{{ index .ExtraValues "CIDRSecondOctet" }}.16.0/20
          - availabilityZone: c
            cidr: 10.{{ index .ExtraValues "CIDRSecondOctet" }}.32.0/20
        isPublic: false
        tags:
          subnet.giantswarm.io/api-server-elb: 'true'
          subnet.giantswarm.io/bastion: 'true'
          subnet.giantswarm.io/control-plane: 'true'
          subnet.giantswarm.io/endpoints: 'true'
          subnet.giantswarm.io/ingress: 'true'
          subnet.giantswarm.io/tgw: 'true'
          subnet.giantswarm.io/workers: 'true'
    topology:
      mode: GiantSwarmManaged
    vpcMode: private

  controlPlane:
    libVolumeSizeGB: 15
    etcdVolumeSizeGB: 50
    rootVolumeSizeGB: 10
    apiMode: private

  # We need to pass the node pools otherwise the test called "has all the worker nodes running" will fail because it
  # expects to find the number of worker nodes in the helm values, but the value is not there if don't pass it, as it's
  # defaulted in the chart template.
  # @TODO: https://github.com/giantswarm/giantswarm/issues/28063
  nodePools:
    # We are using a name with 10 chars which is the max number of characters allowed by our kyverno policies.
    nodepool-0:
      maxSize: 5
      minSize: 2
      rootVolumeSizeGB: 25
      # This can be probably removed once heartbeats are implemented
      # (https://github.com/aws/aws-node-termination-handler/issues/493), since then we would reduce
      # cluster-aws's defaults to a low value, let's say `heartbeatTimeout: 5m` and `globalTimeout: 30m`.
      awsNodeTerminationHandler:
        heartbeatTimeoutSeconds: 100
      # Speed up the ASG cycle when upgrading to avoid having long running test case
      instanceWarmup: 180

  apps:
    certManager:
      values:
        serviceAccount:
          annotations:
            eks.amazonaws.com/role-arn: "{{ .ClusterName }}-CertManager-Role"
        giantSwarmClusterIssuer:
          acme:
            http01:
              enabled: false
            dns01:
              route53:
                enabled: true
                # TODO Use a variable as soon as it is available.
                region: "eu-north-1"
    clusterAutoscaler:
      values:
        configmap:
          # Prevent cluster-autoscaler from trying to scale down while we're in the middle of our scaling test
          scaleDownUnneededTime: 15m0s
