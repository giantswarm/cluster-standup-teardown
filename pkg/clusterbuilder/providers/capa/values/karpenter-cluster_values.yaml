global:
  metadata:
    name: "{{ .ClusterName }}"
    description: "E2E Test cluster"
    organization: "{{ .Organization }}"

  controlPlane:
    libVolumeSizeGB: 15
    etcdVolumeSizeGB: 50
    rootVolumeSizeGB: 10

  # We need to pass the node pools otherwise the test called "has all the worker nodes running" will fail because it
  # expects to find the number of worker nodes in the helm values, but the value is not there if don't pass it, as it's
  # defaulted in the chart template.
  # @TODO: https://github.com/giantswarm/giantswarm/issues/28063
  nodePools:
    # We are using a name with 10 chars which is the max number of characters allowed by our kyverno policies.
    nodepool-0:
      requirements:
        - key: karpenter.k8s.aws/instance-family
          operator: NotIn
          values:
            - t3
            - t3a
            - t2
        - key: karpenter.k8s.aws/instance-cpu
          operator: In
          values:
            - "4"
            - "8"
            - "16"
            - "32"
        - key: karpenter.k8s.aws/instance-hypervisor
          operator: In
          values:
            - nitro
        - key: kubernetes.io/arch
          operator: In
          values:
            - amd64
        - key: karpenter.sh/capacity-type
          operator: In
          values:
            - spot
            - on-demand
        - key: kubernetes.io/os
          operator: In
          values:
            - linux
      type: karpenter

  apps:
    externalDns:
      values:
        triggerLoopOnEvent: true
    clusterAutoscaler:
      values:
        configmap:
          # Prevent cluster-autoscaler from trying to scale down while we're in the middle of our scaling test
          scaleDownUnneededTime: 15m0s
